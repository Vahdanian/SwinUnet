{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MS Lesion Segmentation - Swin UNETR Training\n",
        "\n",
        "This notebook trains a Swin UNETR model for Multiple Sclerosis lesion segmentation using the ISBI 2015 dataset.\n",
        "\n",
        "## ⚠️ IMPORTANT: Enable GPU First!\n",
        "\n",
        "**Before starting, enable GPU in Colab:**\n",
        "- Go to **Runtime** → **Change runtime type** → Set **Hardware accelerator** to **GPU** → **Save**\n",
        "\n",
        "## Steps:\n",
        "1. Clone the repository\n",
        "2. GPU Setup and Verification\n",
        "3. Mount Google Drive and extract dataset\n",
        "4. Install dependencies\n",
        "5. Run training\n",
        "6. Evaluate model on test data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Clone Repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (or navigate to it if already exists)\n",
        "import os\n",
        "\n",
        "repo_url = 'https://github.com/Vahdanian/SwinUnet.git'\n",
        "repo_dir = 'SwinUnet'\n",
        "\n",
        "if os.path.exists(repo_dir) and os.path.exists(os.path.join(repo_dir, '.git')):\n",
        "    print(f\"Repository already exists at {repo_dir}\")\n",
        "    os.chdir(repo_dir)\n",
        "    print(f\"Changed to repository directory: {os.getcwd()}\")\n",
        "else:\n",
        "    # Clone the repository\n",
        "    import subprocess\n",
        "    subprocess.run(['git', 'clone', repo_url], check=True)\n",
        "    \n",
        "    # Change to the repository directory\n",
        "    os.chdir(repo_dir)\n",
        "    \n",
        "    print(\"Repository cloned successfully!\")\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch and pull latest commits from the repository\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Check if we're in the SwinUnet directory\n",
        "if os.path.exists('.git'):\n",
        "    print(\"Repository found. Fetching latest changes...\")\n",
        "    \n",
        "    # Fetch latest commits\n",
        "    result = subprocess.run(['git', 'fetch', 'origin'], \n",
        "                          capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(\"✓ Fetched latest commits from remote\")\n",
        "    else:\n",
        "        print(f\"⚠ Warning: git fetch failed: {result.stderr}\")\n",
        "    \n",
        "    # Check current branch\n",
        "    branch_result = subprocess.run(['git', 'branch', '--show-current'], \n",
        "                                  capture_output=True, text=True)\n",
        "    current_branch = branch_result.stdout.strip() if branch_result.returncode == 0 else 'main'\n",
        "    \n",
        "    # Pull latest changes\n",
        "    print(f\"Pulling latest changes from {current_branch} branch...\")\n",
        "    pull_result = subprocess.run(['git', 'pull', 'origin', current_branch], \n",
        "                                capture_output=True, text=True)\n",
        "    \n",
        "    if pull_result.returncode == 0:\n",
        "        if 'Already up to date' in pull_result.stdout:\n",
        "            print(\"✓ Repository is already up to date\")\n",
        "        else:\n",
        "            print(\"✓ Successfully pulled latest changes\")\n",
        "            print(\"\\nRecent commits:\")\n",
        "            # Show last 5 commits\n",
        "            log_result = subprocess.run(['git', 'log', '--oneline', '-5'], \n",
        "                                       capture_output=True, text=True)\n",
        "            if log_result.returncode == 0:\n",
        "                print(log_result.stdout)\n",
        "    else:\n",
        "        print(f\"⚠ Warning: git pull failed: {pull_result.stderr}\")\n",
        "        print(\"You may need to resolve conflicts manually\")\n",
        "else:\n",
        "    print(\"⚠ Warning: Not in a git repository. Make sure you've cloned the repository first.\")\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Setup (Important!)\n",
        "\n",
        "**Before proceeding, make sure GPU is enabled in Colab:**\n",
        "\n",
        "1. Go to **Runtime** → **Change runtime type**\n",
        "2. Set **Hardware accelerator** to **GPU** (T4, V100, or A100)\n",
        "3. Click **Save**\n",
        "4. The notebook will restart - re-run the cells above\n",
        "\n",
        "The cell below will verify GPU availability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Setup and Verification\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "# Suppress the FutureWarning about cuda.cudart (it's just a deprecation warning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning, message='.*cuda.cudart.*')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GPU Setup Verification\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check CUDA availability\n",
        "cuda_available = torch.cuda.is_available()\n",
        "\n",
        "if cuda_available:\n",
        "    print(\"✓ CUDA is available!\")\n",
        "    print(f\"✓ GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"✓ CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"✓ Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    \n",
        "    # Get GPU memory info\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"✓ GPU Memory: {gpu_memory:.2f} GB\")\n",
        "    \n",
        "    # Set default device\n",
        "    device = 'cuda'\n",
        "    print(f\"\\n✓ Using device: {device}\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"⚠ WARNING: CUDA is NOT available!\")\n",
        "    print(\"\\nTo enable GPU in Colab:\")\n",
        "    print(\"1. Go to Runtime → Change runtime type\")\n",
        "    print(\"2. Set Hardware accelerator to GPU\")\n",
        "    print(\"3. Click Save and restart the notebook\")\n",
        "    print(\"4. Re-run all cells from the beginning\")\n",
        "    print(\"\\n⚠ Training will be VERY SLOW on CPU!\")\n",
        "    print(\"=\" * 60)\n",
        "    device = 'cpu'\n",
        "\n",
        "# Store device for later use\n",
        "import os\n",
        "os.environ['CUDA_DEVICE'] = device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Mount Google Drive and Extract Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Google Drive mounted successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Paths to the zip files in Google Drive MS2015 folder\n",
        "training_zip_path = '/content/drive/MyDrive/Dataset/MS2015/training_final_v4.zip'\n",
        "testdata_zip_path = '/content/drive/MyDrive/Dataset/MS2015/testdata_website_2016-03-24.zip'\n",
        "\n",
        "# Check if zip files exist\n",
        "if not os.path.exists(training_zip_path):\n",
        "    raise FileNotFoundError(f\"Training dataset not found at {training_zip_path}. Please ensure the file exists.\")\n",
        "\n",
        "if not os.path.exists(testdata_zip_path):\n",
        "    raise FileNotFoundError(f\"Test dataset not found at {testdata_zip_path}. Please ensure the file exists.\")\n",
        "\n",
        "print(f\"Found training dataset at: {training_zip_path}\")\n",
        "print(f\"Training file size: {os.path.getsize(training_zip_path) / (1024**3):.2f} GB\")\n",
        "print(f\"\\nFound test dataset at: {testdata_zip_path}\")\n",
        "print(f\"Test file size: {os.path.getsize(testdata_zip_path) / (1024**3):.2f} GB\")\n",
        "\n",
        "# Create ISBI_2015 directory structure\n",
        "os.makedirs('ISBI_2015', exist_ok=True)\n",
        "\n",
        "def extract_and_organize_zip(zip_path, target_folder, folder_name):\n",
        "    \"\"\"Extract zip file and organize it into the target folder structure.\"\"\"\n",
        "    print(f\"\\nExtracting {folder_name} dataset... This may take a few minutes...\")\n",
        "    \n",
        "    # Extract to a temporary location\n",
        "    temp_dir = f'temp_{folder_name}_extract'\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "    \n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(temp_dir)\n",
        "    \n",
        "    # Find the extracted content\n",
        "    extracted_items = os.listdir(temp_dir)\n",
        "    \n",
        "    if len(extracted_items) == 1:\n",
        "        # Single item extracted (likely a folder)\n",
        "        extracted_path = os.path.join(temp_dir, extracted_items[0])\n",
        "        if os.path.isdir(extracted_path):\n",
        "            # Check if it's already the right structure (e.g., contains 'training' or 'testdata_website')\n",
        "            subdirs = [d for d in os.listdir(extracted_path) if os.path.isdir(os.path.join(extracted_path, d))]\n",
        "            if folder_name == 'training' and any(d.startswith('training') for d in subdirs):\n",
        "                # Move the entire folder\n",
        "                target_path = f'ISBI_2015/{target_folder}'\n",
        "                if os.path.exists(target_path):\n",
        "                    shutil.rmtree(target_path)\n",
        "                shutil.move(extracted_path, target_path)\n",
        "            elif folder_name == 'testdata' and any(d.startswith('test') for d in subdirs):\n",
        "                # Move the entire folder\n",
        "                target_path = f'ISBI_2015/{target_folder}'\n",
        "                if os.path.exists(target_path):\n",
        "                    shutil.rmtree(target_path)\n",
        "                shutil.move(extracted_path, target_path)\n",
        "            else:\n",
        "                # Move contents to target folder\n",
        "                target_path = f'ISBI_2015/{target_folder}'\n",
        "                if os.path.exists(target_path):\n",
        "                    shutil.rmtree(target_path)\n",
        "                os.makedirs(target_path, exist_ok=True)\n",
        "                for item in os.listdir(extracted_path):\n",
        "                    shutil.move(os.path.join(extracted_path, item), target_path)\n",
        "        else:\n",
        "            # Single file extracted\n",
        "            target_path = f'ISBI_2015/{target_folder}'\n",
        "            os.makedirs(target_path, exist_ok=True)\n",
        "            shutil.move(extracted_path, target_path)\n",
        "    else:\n",
        "        # Multiple items extracted - look for training/test folders\n",
        "        target_path = f'ISBI_2015/{target_folder}'\n",
        "        if os.path.exists(target_path):\n",
        "            shutil.rmtree(target_path)\n",
        "        \n",
        "        # Try to find the relevant folder\n",
        "        found_folder = None\n",
        "        for item in extracted_items:\n",
        "            item_path = os.path.join(temp_dir, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                if folder_name == 'training' and item.startswith('training'):\n",
        "                    found_folder = item_path\n",
        "                    break\n",
        "                elif folder_name == 'testdata' and ('test' in item.lower() or 'testdata' in item.lower()):\n",
        "                    found_folder = item_path\n",
        "                    break\n",
        "        \n",
        "        if found_folder:\n",
        "            shutil.move(found_folder, target_path)\n",
        "        else:\n",
        "            # Move all contents to target folder\n",
        "            os.makedirs(target_path, exist_ok=True)\n",
        "            for item in extracted_items:\n",
        "                shutil.move(os.path.join(temp_dir, item), target_path)\n",
        "    \n",
        "    # Clean up temp directory\n",
        "    if os.path.exists(temp_dir):\n",
        "        shutil.rmtree(temp_dir)\n",
        "    \n",
        "    print(f\"{folder_name.capitalize()} dataset extracted successfully!\")\n",
        "\n",
        "# Extract both datasets\n",
        "extract_and_organize_zip(training_zip_path, 'training', 'training')\n",
        "extract_and_organize_zip(testdata_zip_path, 'testdata_website', 'testdata')\n",
        "\n",
        "# Verify the dataset structure\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Verifying dataset structure...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if os.path.exists('ISBI_2015/training'):\n",
        "    print(\"✓ Training directory found\")\n",
        "    training_patients = [d for d in os.listdir('ISBI_2015/training') \n",
        "                        if os.path.isdir(os.path.join('ISBI_2015/training', d)) \n",
        "                        and d.startswith('training')]\n",
        "    print(f\"✓ Found {len(training_patients)} training patients: {training_patients}\")\n",
        "else:\n",
        "    print(\"⚠ Warning: Training directory not found. Please check the dataset structure.\")\n",
        "\n",
        "if os.path.exists('ISBI_2015/testdata_website'):\n",
        "    print(\"✓ Test data directory found\")\n",
        "    test_patients = [d for d in os.listdir('ISBI_2015/testdata_website') \n",
        "                    if os.path.isdir(os.path.join('ISBI_2015/testdata_website', d)) \n",
        "                    and d.startswith('test')]\n",
        "    print(f\"✓ Found {len(test_patients)} test patients: {test_patients}\")\n",
        "else:\n",
        "    print(\"⚠ Warning: Test data directory not found. Please check the dataset structure.\")\n",
        "\n",
        "print(\"\\nDataset extraction and verification completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "print(\"Dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installation\n",
        "import torch\n",
        "import torchvision\n",
        "import nibabel\n",
        "import monai\n",
        "import yaml\n",
        "\n",
        "print(\"✓ PyTorch version:\", torch.__version__)\n",
        "print(\"✓ CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"✓ CUDA version: {torch.version.cuda}\")\n",
        "print(\"✓ MONAI version:\", monai.__version__)\n",
        "print(\"✓ All dependencies verified!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-training GPU check and setup\n",
        "import torch\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# Suppress FutureWarning about cuda.cudart (deprecation warning, not an error)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning, message='.*cuda.cudart.*')\n",
        "\n",
        "# Verify GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"✓ GPU READY FOR TRAINING\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
        "    print(\"=\" * 60)\n",
        "    # Set environment variable for training script\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "else:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"⚠ WARNING: NO GPU DETECTED\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Training will be VERY SLOW on CPU!\")\n",
        "    print(\"\\nTo enable GPU:\")\n",
        "    print(\"1. Go to Runtime → Change runtime type\")\n",
        "    print(\"2. Set Hardware accelerator to GPU\")\n",
        "    print(\"3. Click Save (notebook will restart)\")\n",
        "    print(\"4. Re-run all cells from the beginning\")\n",
        "    print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check dataset structure before training\n",
        "import os\n",
        "\n",
        "data_dir = 'ISBI_2015/training'\n",
        "if os.path.exists(data_dir):\n",
        "    print(f\"Dataset directory: {data_dir}\")\n",
        "    \n",
        "    # List training patients\n",
        "    patients = [d for d in os.listdir(data_dir) \n",
        "                if os.path.isdir(os.path.join(data_dir, d)) and d.startswith('training')]\n",
        "    print(f\"\\nFound {len(patients)} training patients:\")\n",
        "    for patient in sorted(patients):\n",
        "        patient_path = os.path.join(data_dir, patient)\n",
        "        \n",
        "        # Check for modalities\n",
        "        orig_path = os.path.join(patient_path, 'orig')\n",
        "        preprocessed_path = os.path.join(patient_path, 'preprocessed')\n",
        "        masks_path = os.path.join(patient_path, 'masks')\n",
        "        \n",
        "        print(f\"\\n  {patient}:\")\n",
        "        if os.path.exists(orig_path):\n",
        "            orig_files = [f for f in os.listdir(orig_path) if f.endswith('.nii.gz')]\n",
        "            print(f\"    - Original files: {len(orig_files)}\")\n",
        "        if os.path.exists(preprocessed_path):\n",
        "            preprocessed_files = [f for f in os.listdir(preprocessed_path) if f.endswith('.nii')]\n",
        "            print(f\"    - Preprocessed files: {len(preprocessed_files)}\")\n",
        "        if os.path.exists(masks_path):\n",
        "            mask_files = [f for f in os.listdir(masks_path) if f.endswith('.nii')]\n",
        "            print(f\"    - Mask files: {len(mask_files)}\")\n",
        "else:\n",
        "    print(f\"⚠ Error: Dataset directory not found at {data_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Ensure we're in the right directory\n",
        "if not os.path.exists('scripts/train.py'):\n",
        "    print(\"Error: train.py not found. Make sure you're in the SwinUnet directory.\")\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Run training with the default configuration\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "!python scripts/train.py --config config/training_config.yaml\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Evaluate Model on Test Data\n",
        "\n",
        "After training, evaluate the model on test data to see the performance metrics:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained model on test data\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import yaml\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, os.getcwd())\n",
        "\n",
        "from src.models import SwinUNETR\n",
        "from src.data import TestDataset, MSLesionDataset\n",
        "from src.evaluation import dice_score, sensitivity, specificity, compute_all_metrics\n",
        "\n",
        "# Configuration\n",
        "config_path = 'config/training_config.yaml'\n",
        "test_data_dir = 'ISBI_2015/testdata_website'  # Test data directory\n",
        "output_dir = 'outputs/experiment_01'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load configuration\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Find the best model checkpoint\n",
        "model_files = []\n",
        "if os.path.exists(output_dir):\n",
        "    for file in os.listdir(output_dir):\n",
        "        if file.endswith('.pth') or file.endswith('.pt'):\n",
        "            model_files.append(os.path.join(output_dir, file))\n",
        "\n",
        "if not model_files:\n",
        "    print(\"⚠ Warning: No model checkpoint found. Make sure training completed successfully.\")\n",
        "    print(f\"Looking in: {output_dir}\")\n",
        "else:\n",
        "    # Use the best model (usually named 'best_model.pth' or similar)\n",
        "    best_model = None\n",
        "    for model_file in model_files:\n",
        "        if 'best' in model_file.lower():\n",
        "            best_model = model_file\n",
        "            break\n",
        "    \n",
        "    if best_model is None:\n",
        "        # Use the most recent checkpoint\n",
        "        best_model = max(model_files, key=os.path.getmtime)\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"Evaluating Model on Test Data\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Model: {best_model}\")\n",
        "    print(f\"Test data: {test_data_dir}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Load model configuration\n",
        "    model_config_path = config['model']['config_path']\n",
        "    with open(model_config_path, 'r') as f:\n",
        "        model_config = yaml.safe_load(f)\n",
        "    \n",
        "    model_params = model_config['model']\n",
        "    \n",
        "    # Create model\n",
        "    print(\"\\nLoading model...\")\n",
        "    model = SwinUNETR(\n",
        "        in_channels=model_params['in_channels'],\n",
        "        out_channels=model_params['out_channels'],\n",
        "        img_size=tuple(model_params['img_size']),\n",
        "        feature_size=model_params['feature_size'],\n",
        "        use_attention=model_params['use_attention'],\n",
        "        attention_type=model_params.get('attention_type', 'cbam')\n",
        "    )\n",
        "    \n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(best_model, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(\"✓ Model loaded successfully\")\n",
        "    \n",
        "    # Check if test data exists\n",
        "    if not os.path.exists(test_data_dir):\n",
        "        print(f\"\\n⚠ Warning: Test data directory not found at {test_data_dir}\")\n",
        "        print(\"Trying to use training data for evaluation instead...\")\n",
        "        test_data_dir = 'ISBI_2015/training'\n",
        "    \n",
        "    # Create test dataset\n",
        "    print(f\"\\nLoading test data from: {test_data_dir}\")\n",
        "    try:\n",
        "        # Try TestDataset first (for test data without ground truth)\n",
        "        dataset = TestDataset(\n",
        "            data_dir=test_data_dir,\n",
        "            use_preprocessed=config['data']['use_preprocessed'],\n",
        "            normalize=config['data']['normalize'],\n",
        "            augmentation=False,\n",
        "            target_size=tuple(config['data']['target_size']) if config['data'].get('target_size') else None,\n",
        "            modalities=config['data']['modalities']\n",
        "        )\n",
        "        has_ground_truth = False\n",
        "        print(\"Using TestDataset (no ground truth masks)\")\n",
        "    except:\n",
        "        # Fall back to MSLesionDataset (with ground truth)\n",
        "        dataset = MSLesionDataset(\n",
        "            data_dir=test_data_dir,\n",
        "            use_preprocessed=config['data']['use_preprocessed'],\n",
        "            normalize=config['data']['normalize'],\n",
        "            augmentation=False,\n",
        "            target_size=tuple(config['data']['target_size']) if config['data'].get('target_size') else None,\n",
        "            modalities=config['data']['modalities']\n",
        "        )\n",
        "        has_ground_truth = True\n",
        "        print(\"Using MSLesionDataset (with ground truth masks)\")\n",
        "    \n",
        "    # Create data loader\n",
        "    test_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config['data']['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=0,  # Use 0 for Colab to avoid issues\n",
        "        pin_memory=False\n",
        "    )\n",
        "    \n",
        "    print(f\"Test samples: {len(dataset)}\")\n",
        "    \n",
        "    # Evaluate\n",
        "    print(\"\\nEvaluating model on test data...\")\n",
        "    all_metrics = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images = batch['image'].to(device)\n",
        "            masks = batch.get('mask')\n",
        "            \n",
        "            if masks is not None:\n",
        "                masks = masks.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            \n",
        "            # Compute metrics if ground truth available\n",
        "            if masks is not None:\n",
        "                for i in range(predictions.shape[0]):\n",
        "                    pred = predictions[i].cpu().numpy()\n",
        "                    mask = masks[i].cpu().numpy()\n",
        "                    metrics = compute_all_metrics(pred, mask)\n",
        "                    all_metrics.append(metrics)\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if all_metrics:\n",
        "        # Aggregate metrics\n",
        "        metric_names = ['dice', 'sensitivity', 'specificity', 'hausdorff_distance']\n",
        "        aggregated = {}\n",
        "        \n",
        "        for metric_name in metric_names:\n",
        "            values = [m[metric_name] for m in all_metrics \n",
        "                     if metric_name in m and not np.isinf(m[metric_name])]\n",
        "            if values:\n",
        "                aggregated[metric_name] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values),\n",
        "                    'min': np.min(values),\n",
        "                    'max': np.max(values)\n",
        "                }\n",
        "        \n",
        "        # Print results\n",
        "        print(f\"\\nNumber of test samples: {len(all_metrics)}\")\n",
        "        print(\"\\nMetrics:\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        if 'dice' in aggregated:\n",
        "            print(f\"Dice Score:        {aggregated['dice']['mean']:.4f} ± {aggregated['dice']['std']:.4f}\")\n",
        "            print(f\"                  Range: [{aggregated['dice']['min']:.4f}, {aggregated['dice']['max']:.4f}]\")\n",
        "        \n",
        "        if 'sensitivity' in aggregated:\n",
        "            print(f\"Sensitivity:       {aggregated['sensitivity']['mean']:.4f} ± {aggregated['sensitivity']['std']:.4f}\")\n",
        "            print(f\"                  Range: [{aggregated['sensitivity']['min']:.4f}, {aggregated['sensitivity']['max']:.4f}]\")\n",
        "        \n",
        "        if 'specificity' in aggregated:\n",
        "            print(f\"Specificity:       {aggregated['specificity']['mean']:.4f} ± {aggregated['specificity']['std']:.4f}\")\n",
        "            print(f\"                  Range: [{aggregated['specificity']['min']:.4f}, {aggregated['specificity']['max']:.4f}]\")\n",
        "        \n",
        "        if 'hausdorff_distance' in aggregated:\n",
        "            print(f\"Hausdorff Distance: {aggregated['hausdorff_distance']['mean']:.4f} ± {aggregated['hausdorff_distance']['std']:.4f} mm\")\n",
        "            print(f\"                  Range: [{aggregated['hausdorff_distance']['min']:.4f}, {aggregated['hausdorff_distance']['max']:.4f}] mm\")\n",
        "        \n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        # Per-sample results (show first few)\n",
        "        print(f\"\\nPer-sample results (showing first 5):\")\n",
        "        print(\"-\" * 60)\n",
        "        for i, metrics in enumerate(all_metrics[:5]):\n",
        "            print(f\"\\nSample {i+1}:\")\n",
        "            for key, value in metrics.items():\n",
        "                if not np.isinf(value):\n",
        "                    if key == 'hausdorff_distance':\n",
        "                        print(f\"  {key}: {value:.4f} mm\")\n",
        "                    else:\n",
        "                        print(f\"  {key}: {value:.4f}\")\n",
        "        \n",
        "        if len(all_metrics) > 5:\n",
        "            print(f\"\\n... and {len(all_metrics) - 5} more samples\")\n",
        "    else:\n",
        "        print(\"\\n⚠ No ground truth available for evaluation.\")\n",
        "        print(\"Predictions were generated but metrics cannot be computed.\")\n",
        "        print(\"To get metrics, use test data with ground truth masks.\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Evaluation completed!\")\n",
        "    print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Monitor Training Progress\n",
        "\n",
        "You can monitor training progress using TensorBoard (if enabled in config):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load TensorBoard extension (if available)\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard (adjust path if needed)\n",
        "# %tensorboard --logdir outputs/experiment_01\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results to Google Drive (Optional)\n",
        "\n",
        "After training completes, you can copy the results to Google Drive for persistence:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy training outputs to Google Drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "output_dir = 'outputs/experiment_01'\n",
        "drive_output_dir = '/content/drive/MyDrive/SwinUnet_Results'\n",
        "\n",
        "if os.path.exists(output_dir):\n",
        "    print(f\"Copying results from {output_dir} to Google Drive...\")\n",
        "    \n",
        "    # Create destination directory if it doesn't exist\n",
        "    os.makedirs(drive_output_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy the entire output directory\n",
        "    shutil.copytree(output_dir, os.path.join(drive_output_dir, 'experiment_01'), dirs_exist_ok=True)\n",
        "    \n",
        "    print(f\"✓ Results saved to {drive_output_dir}\")\n",
        "    \n",
        "    # List saved files\n",
        "    saved_files = os.listdir(os.path.join(drive_output_dir, 'experiment_01'))\n",
        "    print(f\"\\nSaved files:\")\n",
        "    for file in saved_files:\n",
        "        file_path = os.path.join(drive_output_dir, 'experiment_01', file)\n",
        "        if os.path.isfile(file_path):\n",
        "            size_mb = os.path.getsize(file_path) / (1024**2)\n",
        "            print(f\"  - {file} ({size_mb:.2f} MB)\")\n",
        "else:\n",
        "    print(f\"⚠ Output directory not found at {output_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
