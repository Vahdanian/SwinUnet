# Model configuration for Swin UNETR - Optimized

model:
  name: "swin_unetr"
  in_channels: 4  # FLAIR, T1, PD, T2
  out_channels: 1  # Binary segmentation
  img_size: [64, 64, 64]  # Input volume size (H, W, D) - reduced for memory
  feature_size: 32  # Base feature size for Swin UNETR - reduced from 48 to save memory
  use_attention: true  # Enable attention mechanisms
  attention_type: "cbam"  # "cbam" or "multiscale"

# Swin Transformer parameters
swin:
  depths: [2, 2, 2, 2]  # Depths of each stage
  num_heads: [3, 6, 12, 24]  # Number of attention heads
  window_size: [7, 7, 7]  # Window size for shifted windows
  drop_rate: 0.1  # Small dropout for regularization
  attn_drop_rate: 0.1  # Attention dropout
  dropout_path_rate: 0.1  # Drop path for regularization

