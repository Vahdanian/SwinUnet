# Training configuration

# Data settings
data:
  data_dir: "ISBI_2015/training"
  use_preprocessed: true
  normalize: true
  augmentation: true
  target_size: [96, 96, 96]  # Crop/pad to this size
  modalities: ["flair", "mprage", "pd", "t2"]
  batch_size: 2  # Adjust based on GPU memory
  num_workers: 4
  train_split: 0.8  # 80% for training, 20% for validation

# Model settings
model:
  config_path: "config/model_config.yaml"

# Training settings
training:
  num_epochs: 100
  learning_rate: 0.0001
  weight_decay: 0.0001
  optimizer: "adamw"  # "adamw", "adam", or "sgd"
  scheduler: "cosine"  # "cosine", "plateau", or "step"
  scheduler_params:
    T_max: 100  # For cosine annealing
    eta_min: 0.00001
  early_stopping_patience: 10

# Loss function settings
loss:
  type: "combined"  # "dice", "combined", or "focal"
  dice_weight: 0.5
  bce_weight: 0.5
  smooth: 1e-5

# Output settings
output:
  output_dir: "outputs/experiment_01"
  save_best: true
  save_checkpoints: true
  checkpoint_interval: 10  # Save checkpoint every N epochs

# Device settings
device: "cuda"  # "cuda" or "cpu"

# Logging
logging:
  use_tensorboard: true
  log_interval: 10  # Log every N batches

