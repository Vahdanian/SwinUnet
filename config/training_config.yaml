# Training configuration - Optimized for better Dice scores

# Data settings
data:
  data_dir: "ISBI_2015/training"
  use_preprocessed: true
  normalize: true
  augmentation: true
  target_size: [64, 64, 64]  # Reduced from 96 to save memory
  modalities: ["flair", "mprage", "pd", "t2"]
  batch_size: 1  # Reduced to 1 to save memory (use gradient accumulation for effective batch size)
  num_workers: 4
  train_split: 0.8  # 80% for training, 20% for validation

# Model settings
model:
  config_path: "config/model_config.yaml"

# Training settings
training:
  num_epochs: 300
  learning_rate: 0.0002  # Slightly higher initial LR
  weight_decay: 0.0001
  optimizer: "adamw"  # "adamw", "adam", or "sgd"
  scheduler: "cosine_warmup"  # "cosine", "cosine_warmup", "plateau", or "step"
  warmup_epochs: 10  # Warmup for 10 epochs
  scheduler_params:
    T_max: 290  # Cosine annealing after warmup
    eta_min: 1e-6  # Minimum learning rate
  early_stopping_patience: 30  # Increased patience
  gradient_clip_val: 1.0  # Gradient clipping for stability
  use_amp: true  # Mixed precision training (faster, larger effective batch size)
  gradient_accumulation_steps: 8  # Accumulate gradients over N batches (reduces memory, effective batch size = batch_size * gradient_accumulation_steps = 8)
  empty_cache: true  # Clear CUDA cache between batches (helps with memory fragmentation)
  use_gradient_checkpointing: true  # Enable gradient checkpointing in model (trades compute for memory)

# Loss function settings - Enhanced combined loss
loss:
  type: "enhanced"  # "dice", "combined", or "enhanced"
  dice_weight: 0.3
  tversky_weight: 0.3
  bce_weight: 0.2
  focal_weight: 0.2
  smooth: 1e-5
  tversky_alpha: 0.7  # Penalize false positives more
  tversky_beta: 0.3   # Penalize false negatives less
  focal_alpha: 0.25
  focal_gamma: 2.0
  compute_class_weights: true  # Automatically compute class weights for imbalanced data

# Output settings
output:
  output_dir: "outputs/experiment_01"
  save_best: true
  save_checkpoints: true
  checkpoint_interval: 10  # Save checkpoint every N epochs

# Device settings
device: "cuda"  # "cuda" or "cpu"

# Logging
logging:
  use_tensorboard: true
  log_interval: 10  # Log every N batches

